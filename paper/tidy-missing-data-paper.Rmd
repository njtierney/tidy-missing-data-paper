---
documentclass: jss
author:
  - name: Nicholas Tierney
    affiliation: Monash University
    address: >
      Department of Econometrics and Business Statistics
      E774, Menzies Building
      Monash University, Clayton
    email: \email{nicholas.tierney@gmail.com}
    url: http://www.njtierney.com
  - name: Dianne Cook
    affiliation: Monsh University
title:
  formatted: "Expanding tidy data principles to facilitate missing data exploration, visualization and assessment of imputations"
  # If you use tex in the formatted title, also supply version without
  plain:     "Expanding tidy data principles to facilitate missing data exploration, visualization and assessment of imputations"
  # For running headers, if needed
  short:     "Explore missingness with \\pkg{naniar}"
abstract: >
  Despite the large body of research on missing value distributions and imputation, there is comparatively little literature with a focus on how to make it easy to handle, explore, and impute missing values in data. This paper addresses this gap. The new methodology builds upon tidy data principles, with the goal of integrating missing value handling as an integral part of data analysis workflows. We define a new data structure, and a suite of new operations. Together, these provide a cohesive framework for handling, exploring, and imputing missing values. These methods have been made available in the R package called `naniar`.
keywords:
  # at least one keyword must be supplied
  formatted: [statistical computing, statistical graphics, data science, data visualization, tidyverse, data pipeline, "\\proglang{R}"]
  plain:     [statistical computing, statistical graphics, data science, data visualization, tidyverse, data pipeline, R]
preamble: >
  \usepackage{amsmath}
  \usepackage[english]{babel}
output:
  bookdown::pdf_book:
    base_format: rticles::jss_article
    includes:
      in_header: header.tex
    keep_tex: yes
    citation_package: 'natbib'
bibliography: tidy-missing-data-paper.bib
# csl: jcgs.csl
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "center",
                      fig.width = 4, 
                      fig.height = 4, 
                      cache = TRUE)

library(tidyverse) 
library(gridExtra)
# options(kableExtra.latex.load_packages = FALSE)
# library(kableExtra)
library(ggthemes)
library(png)
library(scales)
library(simputation)
# if paper doesn't compile, use development version of naniar
# install.packages("remotes")
# remotes::install_github("njtierney/naniar")
library(naniar) 
library(knitr)
library(lubridate)
library(magick)
library(rpart)
library(visdat)
library(patchwork)
options(tinytex.verbose = TRUE)
```

# Introduction {#intro}

As data science has become a more solid field, theories and principles have developed to describe best practices. One such idea is 'tidy data,' which defines of a clean, analysis-ready format, that informs the workflow to convert raw data through a data analysis pipeline [@Wickham2014]. The second idea is the grammar of graphics, which describes how to map data values into a visualization [@Wilkinson2012]. These principles have been widely adopted. However, these principles do not address the problem of missing data. In particular, there is little guidance for missing value handling in the data workflow. Most analysis and visualisation software simply drops missing values when making a plot, although some packages like \pkg{ggplot2} provide a warning (Figure \@ref(fig:warning)).

<!-- Tidy data [@Wickham2014] is a relatively new principle and suite of tools -->
<!-- facilitating the process of converting raw data into a clean, analysis-ready -->
<!-- format that works efficiently in a data analysis pipeline. In tidy data, missing -->
<!-- values can be handled implicitly or explicitly, at the analyst's discretion. -->
<!-- However, when making a plot, missing values are simply dropped, albeit with a -->
<!-- warning (see Figure \@ref(fig:warning)). Most other analysis software will -->
<!-- simply drop cases with missings without warning. -->

```{r warning, fig.cap = "How ggplot2 behaves when displaying missing values. A warning message is displayed, but missing values are not shown in the plot", fig.height = 2.5, fig.width = 2.5, out.width = "75%", warning = FALSE, fig.show = 'hold'}

p1 <- ggplot(oceanbuoys,
       aes(x = humidity,
           y = air_temp_c)) +
  geom_point(size = 0.5) +
  theme(aspect.ratio = 1)

ggsave("images/geom-point-warn.png", 
       width = 4, 
       height = 4, 
       units = c("in"))

img_gg_p <- image_read("images/geom-point-warn.png")

img_gg_warn <- image_read("images/ggplot-warn.jpeg")

image_append(c(img_gg_warn, img_gg_p)) %>% 
  image_write("images/ggplot2-warning-and-plot.png", "png")

knitr::include_graphics("images/ggplot2-warning-and-plot.png")

```

The imputation literature focusses on ensuring valid statistical inference
is made from incomplete data. This is approached chiefly through probabilistic
modelling, assuming the mechanism of missing data is known to the analyst. It
does not address how to explore, understand, and handle missing data structures
and mechanisms.

However, something must be known about the missing value structure to produce a
complete dataset for analysis, whether by case- or variable-deletion, or with
imputation. Decision tree models or latent group analysis can reveal structures or patterns
of missingness [@Tierney2015; @Barnett2017], but are not definitive. While there
might be other times where the missing data mechanism is obvious, determining
this is not straightforward. To understand their structure, the analyst must explore the data
with visualizations, summaries, and modelling. It is an iterative procedure, to arrive at a reasonably confident state of understanding the missing data mechanism. 

While there have been many software tools for exploring missing data, they do not work together. The graphics literature provides several solutions for exploring missings visually by incorporating them into the plot in some way. For example, imputing values to be 10% below the minimum will include all observations into a scatter plot. 

However, this is often implicit, and not transparent to the user. (**NOTE** include possible brief example from mice/mi/VIM). The ideas from the graphics literature need to be translated into tidy data tools to integrate missing data handling in a data pipeline.

This paper is organized in the following way. The next section (\@ref(background)) provides the background to to tidy data principles and tools (\@ref(tidy-data-concepts) and missing data representations (\@ref(missing-data-rep-dep). Section \@ref(existing-software) summarises existing software for handling missing data. The new work extending the tidy tools to facilitate exploring, visualizing, and imputing missing data is discussed in Section \@ref(extensions). An application illustrating the use of the new methods is shown in Section \@ref(case-study). Section \@ref(discussion) discusses strengths, limitations, and future directions.

# Background {#background}

## Tidy data concepts and methods {#tidy-data-concepts}

Features of tidy data were formally discussed in @Wickham2014, and were discussed in terms of their importance for data science by @Donoho2017, and tools for data analysis. Tidy data is defined by @r4ds as:

> 1. Each variable must have its own column.
> 1. Each observation must have its own row.
> 1. Each value must have its own cell.

Tidy data is easier to work with and analyze because the variables are in the same format as they would be put into modelling software. This helps the analyst works swiftly and clearly, closing up opportunities for errors. Tidy data principles are general, but are comprehensively implemented in the R programming language, so this paper focusses on R.

Tidy tools require the same tidy data input and output. This consistency means multiple tools can be composed together into a sequence, allowing for rapid, elegant, and complex operations. Contrasting tidy tools are messy tools. These have tidy input but messy output. Messy tools slow down analysis by shifting the focus from analysis to transforming output so it is the right shape for the next step in the analysis. This makes the work at each step harder to predict, and more complex and difficult to maintain. This disrupts workflow, and invites errors. Tidy tools fall into three broad categories: data manipulation, visualization, and modelling.

### Data manipulation {#tidy-data-manip}

Data manipulation is made input- and output-tidy with R packages \pkg{dplyr} and \pkg{tidyr} [@dplyr; @tidyr]. These provide the five "verbs" of data manipulation: data reshaping, sorting, filtering, transforming, and aggregating. **Data reshaping** goes from long to wide formats; **sorting** arranges rows in a specific order; **filtering** removes rows based on a condition; **transforming**, changes existing variables or adds new ones; **aggregating** creates a single value from many values, say, for example, by computing the minimum, maximum, and mean.

### Visualizations {#tidy-vis}

Visualization tools only have tidy data as their input, as the output is a graphic. The popular domain specific language \pkg{ggplot2} maps variables in a dataset to features (referred to as aesthetics) of a graphic [@ggplot2]. For example, a scatterplot can be created by mapping two variables to the x and y axes, and specifying a point geometry. 

### Modelling {#tidy-model}

Modelling tools work well with tidy data, as they have a clear mapping from variables in the data to the formula for a model. For example in R, y regressed on x and z is: \code{lm(y ~ x + z)}. Modelling tools are input tidy, but their output is always messy - it is not in the right format for subsequent steps in analysis. For example, estimated coefficients, predictions, and residuals from one model cannot be easily combined with the output of another model. Messy models have been partially addressed with the \pkg{broom} package, which tidies up model outputs for data analysis, and the developing \pkg{recipes} package, which helps make modelling input- and output-tidy [@recipes].

### The tidyverse {#the-tidyverse}

Defining tidy data and tidy tools has resulted in a growing set of packages known collectively as the "tidyverse" [@tidyverse]. These are constructed to share similar principles in their design and behavior, and cover the breadth of an analysis - from importing, tidying, transforming, visualizing, modelling, to communicating [@r4ds; @tidyverse; @Tidyverse-Manifesto]. This has led to more tools for specific parts of analysis - from reading in data with \pkg{readr}, \pkg{readxl}, and \pkg{haven}, to handling character strings with \pkg{stringr} and dates with \pkg{lubridate}, and performing functional programming with \pkg{purrr} [@readr; @readxl; @haven; @stringr; @lubridate; @purrr]. It has also led to a burgeoning of new packages for other fields following similar design principles, creating fluid workflows for new domains. For example, the \pkg{tidytext} [@tidytext] package for text analysis, the \pkg{tsibble} [@wang2020tsibble] package for time series data, and \pkg{tidycensus} [@tidycensus] for working with US census and boundary data.

### Tidy formats for missing data {#tidy-formats-missing-data}

Current tools for missing data are messy. Missing data tools can be used to perform imputations, missing data diagnostics, and data visualizations. However, these tools suffer the same problems as modelling for imputation: They use tidy input, but produce messy output - their output is challenging to integrate with other steps of data analysis. The complex, often multivariate nature of imputation methods also makes makes them difficult to represent. Visualization methods for missing data do not map data features to the aesthetics of a graphic, as in \pkg{ggplot2}, limiting expressive exploration.

Taking existing methods from the missing data graphics literature, and translating and expanding them into tidy data and tidy tools would create more effective data visualisations.
Defining these concepts allows the focus to be more general than just software, but rather, an extensible framework for tidy tools to explore missing data.

## Missing data representation and dependence {#missing-data-rep-dep}

The convention for representing missingness is a **b**inary matrix, $B$, for data $y$ with $i$ rows and $j$ columns:

$$
b_{ij} =\begin{cases}
1, & \text{if } y_{ij} \text{ is missing} \\
0, & \text{if } y_{ij} \text{ is observed}
\end{cases}
$$

There are many ways each value can be missing, we adopt the notation used in @VanBuuren2012. The information in $B$ can be used to arrive at three categories of missing values: Missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). The distribution of missing values in $b_{ij}$ can depend on the entire dataset, represented as $Y = (Y_{obs}, Y_{miss})$. This relationship can be defined by the _missing data model_ $Pr(r_{ij} | Y_{obs}, Y_{miss}, \psi)$, the probability of missingness is conditional on data observed, data missing, and some probability parameter of missingness, $\psi$. This helps to precisely define categories of missing values.

**MCAR** is where values being missing have no association with observed or unobserved data, that is, $Pr(R = 1 | Y_{obs}, Y_{miss}) = Pr(R = 1 | \psi)$. Essentially, the probability of an observation being missing is unrelated to anything else, only the parameter $\psi$, the overall probability of missingness. Although a convenient scenario, it is not actually possible to confirm, or clearly distinguish from MAR, as it relies on statements on data unobserved. In **MAR**, missingness only depends on data observed, not data missing, that is, $Pr(R = 1 | Y_{obs}, Y_{miss}, \psi) = Pr(R = 1 | Y_{obs}, \psi)$. Some structure or dependence between missing and observed values is allowed, provided it can be explained by data observed, and some overall probability of missingness. In **MNAR**, missingness is related to values observed, and unobserved: $Pr(R = 1 | Y_{obs}, Y_{miss}, \psi)$. This assumes conditioning on all observations: data goes missing due to some phenomena unobserved, including the structure of the missing data itself.  This presents a challenge in analysis, as it is difficult to verify, and implies bias in analysis due to the unobserved phenomena.

<!-- (possibly remove this next para?) -->

Visualizations can help assess whether data is MCAR, MAR or MNAR. Imputation is recommended in most cases of missingness dependence.

# Existing Software {#existing-software}

Methods for exploring, understanding, and imputing missing data are more accessible now than they have ever been. In particular, the number of methods and implementations of imputation are seeing continued growth. Values can be imputed with one value (single imputation), or multiple values (multiple imputation), creating $m$ datasets. This section discusses existing software for single and multiple imputation, and exploration.

(**read over next two sections - Nick**)

## Imputation {#imputation}

\pkg{VIM} [@VIM] implements well-used imputation methods K nearest neighbors, regression, hot-deck, and iterative robust model-based imputation. These diverse approaches allows for imputing with semi-continuous, continuous, count, and categorical data. VIM identifies imputed cases by adding an indicator variable with a suffix `_imp`. So `Var1` has a sibling column, `Var1_imp`, with values TRUE or FALSE indicating imputation. \pkg{VIM} also has a variety of visualization methods, discussed in \@ref(exploration). \pkg{simputation} provides an interface to imputation methods from VIM, in addition providing hotdeck imputation, and the EM algorithm [@simputation; @Dempster1977]. \pkg{simputation} provides a consistent formula interface to all imputation, and always returns a dataframe with the updated imputations. \pkg{Hmisc} [@Hmisc], provide predictive mean matching, \pkg{imputeTS} [@imputeTS], provides time series imputation methods, and \pkg{missMDA} [@missMDA], imputes data using principal components analysis.

Multiple imputation is often regarded as best practice for imputing values [@Schafer2002], as long as appropriate caution is taken [@Sterne2009]. Popular and robust methods for multiple imputation include the \pkg{mice}, \pkg{Amelia}, and \pkg{mi} packages [@mice; @amelia; @mi]. \pkg{mice} implements the method of chained equations, using a variable-wise algorithm to calculate the posterior distribution of parameters to generate imputed values. The workflow in \pkg{mice} revolves around imputing data, returning completed data, and fitting a model and pooling the results. 

\pkg{Amelia} [@amelia] assumes data are multivariate normal, and samples from the posterior using the computationally efficient (and parallelizable) Expectation-Maximization Bootstrap (EMB) algorithm [@Honaker2010] and allows for incorporation of information on the values in a prior. \pkg{norm} [@norm], provides multiple imputation using EM for multivariate normal data, drawing from methods in the NORM software [@schafer-norm]. \pkg{norm} does not provide a framework for tracking missing values, instead providing tools for making inference from multiple imputation. 

\pkg{mi} [@mi] also uses Bayesian models for imputation, providing better handling of semi-continuous values, and data with structural or perfect correlation. A collection of analysis models are also provided in \pkg{mi}, which work with the specially multiply imputed data, including linear models, generalized linear models, and their corresponding Bayesian components. This approach promotes fluid workflow, with a similar penalty to tidying up model output, which is still messy. 

### Summary {#imputation-summary}

Each imputation method provides practical methods for different usecases, but most have different output structures, and do not have consistent interfaces in their implementation. This makes then inherently messy and challenging to integrate into an analysis pipeline. For example, combining different imputation methods from different pieces of software is not currently straightforward. \pkg{simputation} is different from other imputation packages, and resolves some of these complications with a simple approach of a unified syntax for all imputation, and always returning a dataframe containing imputed values. This reduces the friction of working with other tools, but comes at the cost of identifying imputed values. An ideal approach would use consistent, simple data structures that work with other analysis tools, and help track missing values. This would make imputation outputs tidy, streamlining subsequent analysis.

## Exploration {#exploration}

The primary focus of most missing data packages is making inferences, and exploring imputed values, not
on exploring relationships in missing values, and identifying possible patterns.
Texts covering the exploration phase of missing data have the same problem as with modelling: the input is tidy, but the output does not work with other tools [@VanBuuren2012]; this is inefficient. Methods for exploring missing values are primarily covered in literature on interactive graphics [@Swayne1998; @Unwin1996; @Cook2007], and are picked up again in a discussion of a graphical user interface [@Cheng2015]. 

The missingness matrix $B$ can be used to assess missing data dependence It has been used in interactive graphics, dubbed a "shadow matrix", to link missing and imputed values to the data, facilitating their display [@Swayne1998], focusing heavily on multivariate numeric data. This is an idea upon which this new work builds.

The MANET (Missings Are Now Equally Treated) software @Unwin1996 focussed on multivariate categorical data, with missingness explicitly added as a category. MANET also provided univariate visualizations of missing data using linked brushing between a reference plot of the missingness for each variable, and a plot of the data as a histogram or barplot. The MANET software is no longer maintained and cannot be installed. The approach of @Swayne1998 in the software XGobi, further developed in ggobi [@Cook2007], focussed more on multivariate quantitative data. Missingness is incorporated into plots in ggobi by setting them to be 10% below the minimum value. 

MissingDataGUI provides a Graphhical User Interface (GUI) for exploring missing data structure both numerically and visually. Using a GUI to explore missing data facilitates rapid insight into missingness structures. However, this comes as a trade off, as insights are not captured or recorded with a GUI, making it challenging to incorporate into reproducible analyses. This distracts and breaks analysis workflow, inviting mistakes.

VIM (Visualizing and Imputing Missing Data) provides visualization methods to identify and explore observed, imputed, and missing values. These include spinograms, spinoplots, missingness matrices, plotting missingness in the margins of other plots, and other summaries. However, these visualizations do not map variables to graphical aesthetics, creating friction when moving through analysis workflows, making them difficult to extend to new circumstances. Additionally, data used to create the visualizations cannot be accessed, posing a barrier to further exploration.

\pkg{ggplot2} incorporates missingness into visualizations only when mapping a discrete variable to a graph aesthetic. This has some limitations. For example, data of school grades and test scores, a boxplot visualization with school grade and test score mapped to the x- and y-axis (Figure \@ref(fig:gg-box-na)). If there are missings in a continuous variable like test score, \pkg{ggplot2} omits the missings and prints a warning message. However, if a discrete variable like school year has missing values, an NA category is created for school year, where scores are placed (Figure \@ref(fig:gg-box-na)).

```{r make-school-scores-data, echo = FALSE}

school_scores <- data.frame(score = c(rnorm(20, 70, 10),
                                 rnorm(20, 85, 7.5),
                                 rnorm(20, 95, 5),
                                 rnorm(20, 115, 10)),
                       year = rep(c("1st", "2nd", "3rd", "4th"), each = 20)) %>%
  mutate(score = as.numeric(score),
         year = as.factor(year)) %>%
  as_tibble()

school_scores_year_miss <- school_scores %>%
  mutate_at(.funs = function(x){
    x[sample(c(TRUE, NA), 
             prob = c(0.85, 0.150), 
             size = length(x), 
             replace = TRUE)]
  },
  .vars = "year")


school_scores_score_miss <- school_scores %>%
  mutate_at(.funs = function(x){
    x[sample(c(TRUE, NA), 
             prob = c(0.85, 0.150), 
             size = length(x), 
             replace = TRUE)]
  },
  .vars = "score")


school_scores_both_miss <- school_scores %>%
  mutate_all(.funs = function(x){
    x[sample(c(TRUE, NA), 
             prob = c(0.85, 0.150), 
             size = length(x), 
             replace = TRUE)]
  })

school_scores_all <- bind_rows(
  complete = school_scores,
  year_NA = school_scores_year_miss,
  score_NA = school_scores_score_miss,
  both_NA = school_scores_both_miss,
  .id = "NA_type"
)


```


```{r gg-box-na, fig.height = 3, fig.width = 9, out.width = "100%", fig.align = "center", fig.cap = "ggplot2 provides different visualizations depending on what type of data has missing values for data of student test scores in school year. (A) Data is complete; (B) Missings are only in year - an additional 'NA' boxplot is created; (C) Missings only in scores, no additional missingness information is shown; (D) Missings in both scores and year, additional missing information is shown. The missingness category is only shown when there are missings in categorical variables such as year (plots (B) and (D)). In (C), no missingness information is given on the graphic, despite there being missings in score, and a warning message is displayed about the number of missing values omitted.", fig.show = "asis"}

gg_boxplot <-  function(data){
  ggplot(data,
       aes_string(x = "year",
                  y = "score")) +
  geom_boxplot()
}

p1 <- gg_boxplot(school_scores_all %>% filter(NA_type == "complete"))
p2 <- gg_boxplot(school_scores_all %>% filter(NA_type == "year_NA"))
p3 <- gg_boxplot(school_scores_all %>% filter(NA_type == "score_NA"))
p4 <- gg_boxplot(school_scores_all %>% filter(NA_type == "both_NA"))

cowplot::plot_grid(p1,
          p2,
          p3,
          p4,
          nrow = 1,
          labels = LETTERS[1:4])
```

# Tidy framework for missings {#extensions}

Applying tidyverse principles has the potential to clarify missing data exploration, visualization, and imputation. This
section discusses how these principles are applied for data structures \@ref(data-structure), common operations (verbs) \@ref(verbs), graphics \@ref(graphics), and data summaries \@ref(num-sum). The design and naming of these features is important, and care has been taken to make the names intuitive for their purpose.

## Data structure {#data-structure}

(note: section to work on this week.)

To explore missing data there needs to be a robust representation of missing
values that integrates into analysis pipelines. This means a data structure that is easily transported, and not a special class reliant on attributes that can be lost in analysis. The $B$ matrix, where 0 and 1 indicate not missing and missing (respectively), provides a useful template, as it is very common in missing data literature.  This matrix was used to explore missing values in
the interactive graphics library XGobi, called a "missing value
shadow", or "shadow matrix", defined as a copy of the original data with
indicator values of missingness. The shadow matrix could be interactively linked to the data. There are some limitations to the shadow matrix, however. Namely, the values 0 and 1 can be confusing representations of missing values - does 0 indicate an absence of observation, or the presence of a missing value? 


```{r shadow-matrix-progress, echo = FALSE, fig.cap = "Series of steps to create shadow matrix data. (1-2) Data to a binary representation of missings, (2-3) Binary format converted to a shadow matrix (3-4) New and improved shadow matrix with changed variable names. This clearly links a variable to its state of missingness.", out.width = "100%", fig.align="center"}

knitr::include_graphics("images/full-conversion-to-shadow.png")

```

We propose a new form for tidy representation of missing data based on these ideas from past research. Four features are added to the shadow matrix to facilitate analysis of missing data, illustrated in Figure \@ref(fig:shadow-matrix-progress).

(**note** - condense details below into points here.)



1. **Missing value labels**. Simple labels, "NA" and "!NA", for missing and not missing, clearly identifies missing values for analysis and plotting. This is an improvement over the commonly used binary matrix ($B$), which makes it unclear if 0 indicates the absence (0) of observation, or the presence (1) of a missing value. Missing value labels can be represented as "factor" types values, with text values "NA" and "!NA" and underlying number values 1 and 0, for missing and not missing (Figure \@ref(fig:shadow-matrix-progress) parts 1-3). (principle: clarity of labelling - transparent what the matrix means, because these could equally be "missing" or "present" - for this. Anybody looking at these values)

1. **Special missing values**: Building on **missing value labels**, the values in the shadow matrix can be "special" missing values, indicated by "NA_\<suffix\>" - for example: `NA_instrument` uses a short label, "instrument", indicating instrument error resulting in missing values. These could be also used to indicate imputations, (Table \@ref(tab:shadow-encoding)). The underlying number representation changes from binary to integer, allowing for unlimited missingness features. For example, instrument failure and drop out would be represented as 2 and 3 (Table \@ref(tab:shadow-encoding)). Encoding special missing values is achieved by defining logical conditions and suffixes with the `recode_shadow` function in \pkg{naniar} (\@ref(verbs-recode)). {Other statistical programs (SPSS, SAS, and Stata) represent missingness values as a full-stop, `.`, and allow for recording special missing values as `.a` - `.z`.  SAS, SPSS, and STATA all implement } {The special values from these languages break the rule of "keeping one thing in a cell", as they record both the value and the multivariate missingness state. Additionally, special missing values in other statistical programming languages are encoded at a lower level in the fundamental `NA` type, which is not possible in R.} {(discuss the principles, rather than what we've done)}. {(in the same column you have mixed information, which breaks the tidy principle of one column having one type of value)}.

1. **Coordinated names**: Variables in the shadow matrix gain a consistent short suffix, "_NA", keeping names coordinated throughout analysis (Figure \@ref(fig:shadow-matrix-progress) part 4). The suffix is short and easy to remember in analysis or visualization. This shifts the focus from the value of a variable to its missingness state, making intent clear when performing analysis.

(It makes a clear distinction with `var_NA` being a random variable of the missingness of a variable, `var`. )

(change cohesiveness to connectedness)

1. **Connectedness**: Binding the shadow matrix column-wise to the original data creates a single connected _nabular_ data format, so that it cannot be out of sync with the data. useful for visualization, summaries, and tracking imputed values. This is discussed in more detail below in \@ref(nabular-data).


```{r shadow-encoding}

fig_shadow_code <- tibble::tibble(temp = c(-99, NA, -1, 106),
                                  temp_NA = c("NA_instr",NA, "NA_dropout","!NA"),
                                  NA_value = c(2, 1, 3, 0))

knitr::kable(fig_shadow_code,
             caption = "Example data of temperature, its shadow representation, and underlying NA value. The temperature value of -99 is represented in the shadow column as 'NA\\_instr', which in turn has the underlying numeric value of 2. This captures additional information about the data otherwise difficult to record.")
             # booktabs = TRUE) 
  # kable_styling(latex_options = c("hold_position"))

```


## Nabular data {#nabular-data}

_Nabular_ data binds the shadow matrix column-wise to the original data. It is a portmanteau of `NA` and `tabular`. _Nabular_ data keeps corresponding rows together, removing the possibility of mismatching records, explicitly linking missing values to data. _Nabular_ data facilitates visualization and summaries by allowing the user to reference the missingness of a variable, `var`, as `var_NA`. _Nabular_ data is a snapshot of the missingness of the data. This means when _nabular_ data is imputed, those imputed values can easily be identified in analysis by referring to the coordinated names. _Nabular_ data is created with `nabular(data)` (Figure \@ref(fig:nabularfig)). _Nabular_ data is not unlike classical data formats with quality or flag columns associated with each measured variable, e.g. Scripps CO2 data [@Keeling2005-scripps], GHCN data [@Durre2008-ghcn]. 


```{r nabularfig, echo = FALSE, fig.cap = "The process of creating nabular data. Data transformed to shadow matrix, and nabular data contains the shadow matrix, column bound to the data. Nabular data can be created using `bind\\_shadow` or `nabular()` functions. Nabular data provides a useful format for missing data exploration and analysis.", out.width = "100%", fig.align = "center"}

knitr::include_graphics("images/nabular.png")

```

The _nabular_ data structure provides three key benefits over on-the-fly creating a missingness matrix $B$. Firstly, missing value labels `NA` and `!NA` are clearer than TRUE or FALSE. Secondly, special missing values cannot be added easily during analysis with a logical matrix. Finally, the logical matrix cannot capture which values are imputed, if imputation has already taken place. Imputing values on _nabular_ data automatically tracks these value changes.

Using additional columns to represent missingness information follows recently published best practices for data organization, described in @Ellis2017 and @Broman2017: (1) Keep one thing in a cell and (2) Describe additional features of variables in a second column. Here they suggest to indicate censored data with an extra variable called "VariableNameCensored", which would be TRUE if censored, otherwise FALSE. This information can now be represented in the shadow columns as special missings.

## Missing data operations {#verbs}

Common missing data operations can be considered verbs, in the tidyverse sense. For missing data, these include: **scan**, **replace**, **add**, **shadow**, **impute**, **track**, and **flag**. Data can be **scanned** to find possible missings not coded as `NA`. These values can then be **replaced** with `NA`. To facilitate exploration, summaries of missingness can be **added** as a summary column to the original data. Data matrix can be augmented with the **shadow matrix** values, helping explore missing data, as well as facilitating the process of **imputing**, and **tracking**. Finally, unusual or specially coded missing values can be **flagged**.

### scan: Searching for common missing value labels  {#verbs-search}

This operation is used to search the data for specific conventional representations of missings, such as, "N/A", "MISSING", `-99`. 
The function is called  `miss_scan_count()`, and it returns a table of occurrences of that value for each variable. Two more data objects, `common_na_numbers` and `common_na_strings` provide a list of common NA values for numbers and characters, to help users check for the typical representations of missing.

```{r setup-miss-scan-count}

dat_ms <- tibble::tribble(~x,  ~y,    ~z,
                         1,   "A",   -100,
                         3,   "N/A", -99,
                         NA,  NA,    -98,
                         -99, "E",   -101,
                         -98, "F",   -1)
```


```{r miss-scan-count, eval = FALSE}
miss_scan_count(data = dat_ms, 
                search = -99) %>% 
  knitr::kable(
    caption = "Table of the occurences of the search '-99' in the data, 'dat\\_ms'. There is one occurence if -99 in variables x and z.")
    # booktabs = TRUE) 
  # kable_styling(latex_options = c("hold_position"))
```


### replace: Replacing missing value label with another {#verbs-replace-with}

Once the different representations of possible missing values have been identified, these values can be replaced using the function, `replace_with_na()`.  A dataset could have the values `-99` in the `x` column replaced with the following code:
`replace_with_na(dat_ms, replace = list(x = -99))`. For operating on multiple variables, there are scoped variants for `replace_with_na`: `_all`, `_if`, and `_at`. This means `replace_with_na_all` operates on **all** columns, `replace_with_na_at` operates **at** specific columns, and `replace_with_na_if` makes a conditional change on columns **if** they meet some condition (such as `is.numeric` or `is.character`). 

### add: Adding missingness summary variables {#verbs-add-cols}

Summary information such as the number of missings in a given case can be useful in understanding missingness structure, and even more so when this information is kept alongside the data. Functions starting with `add_` exist in `dplyr` to add count information for particular groups or conditions to the data. `naniar` includes functions to add missingness summary information to the data, such as the number or proportion of missingness, the missingness cluster, or if there are any missings, using: `add_n_miss()`, `add_prop_miss()`, `add_miss_cluster`, and `add_any_miss()`, respectively (Table \@ref(tab:add-missing-info)). There are also functions for adding information about shadow values or nice labels for any missing values with `add_label_shadow()` and `add_label_missings()`. An example use of these features is in @Tierney2015, where the proportion of
missings is used as the outcome in a model, to identify variables and values
important in predicting missingness structures.

```{r add-missing-info}

tibble::tibble(
  `Function` = c(
    "add_n_miss(data)",
    "add_prop_miss(data)",
    "add_miss_cluster(data)"
    ),
  `Adds column which:` = c(
    "Contains the number missing values in a row",
    "Contains the proportion of missing values in a row",
    "Contains the missing value cluster"
  )) %>% 
    knitr::kable(caption = "Overview of the 'add' functions in naniar")

```


### shadow: Creating nabular data {#verbs-nabular}

_Nabular_ data has the shadow matrix column-bound to existing data. This facilitates visualization and summaries, and allows for imputed values to be tracked. _Nabular_ data can be created with `nabular()`, as below:

```{r bind-shadow, echo = TRUE}
nabular(dat_ms)
```

### flag: Describing different types of  missing values {#verbs-recode}

Unusual or spurious data values are often identified and `flagged`. For example, there might be special codes to mark an individual dropping out of a study, known instrument failure in weather instruments, or for values censored in analysis. These special types of missingness can be encoded in the shadow matrix of  _nabular_  data, using `recode_shadow()`. Using `recode_shadow()` requires specifying the variable you want to contain the flagged value, the condition you want this to occur at, and a suffix for the new type of missing value. This is then recoded as a new factor level in the shadow matrix, so every column is aware of all possible new values of missingness. For example, the values -99 and -98 could be recoded to mean a broken machine sensor for the variable x:

```{r recode-shadow, eval = TRUE, echo = TRUE}
nabular(dat_ms) %>%
  recode_shadow(x = .where(x == -99 ~ "broken_sensor"))
```

### impute: Imputing values {#verbs-impute}

`naniar` does not reinvent the wheel for imputation, as there are many robust R packages for imputation \@ref(imputation). Instead, _nabular_ data is designed to work with existing imeplementation. `naniar` does provide a few imputation methods to facilitate exploration and visualization: `impute_mean`, `impute_median`, and `impute_below`. While useful to explore structure in missingness, they are not recommended for use in analysis. `impute_below` imputes values below the minimum value, with some jitter (small random noise) to reduce overplotting. It has options to change the default amount of shift, and jitter. 

```{r demonstrate-impute-below, eval = FALSE}
airquality %>%
  impute_below_at(vars(Ozone)) %>%
  select(Ozone, Solar.R) %>%
  head()
```

Similar to `simputation`, each `impute_` function returns the data with values imputed. However, `naniar` does not use a formula syntax, instead each function has "scoped variants" `_all`, `_at` and `_if` as in \@ref(verbs-replace-with). `impute_` functions with no scoped variant, (`impute_mean`), will work on a single vector, but not a data.frame. One challenge with this approach is imputed value locations are not tracked. This issue is resolved with \code{nabular} data covered in Section \@ref(verbs-track).

```{r impute-vector, eval = FALSE}

impute_mean(airquality$Ozone) %>% head()

impute_mean_at(airquality, .vars = vars(Ozone)) %>% head()

impute_mean_if(airquality, .predicate = is.integer) %>% head()

impute_mean_all(airquality) %>% head()

```

### track: Shadow and impute missing values {#verbs-track}

To evaluate imputations they need to be tracked, which is achieved by combining the verbs `nabular`, `impute_`, and `add_label_shadow`. The missing values can then be referred to by their shadow variable, `_NA`. The missingness of any observation can be referred to with `any_missing` (Figure \@ref(fig:track-impute-example)). The code chunk below shows the track pattern, first using `nabular`, then imputing with `impute_lm`, and adding a `label_shadow`:

```{r bind-impute-label-example, echo = TRUE}
aq_imputed <- nabular(airquality) %>%
  simputation::impute_lm(Ozone ~ Temp + Wind) %>%
  simputation::impute_lm(Solar.R ~ Temp + Wind) %>%
  add_label_shadow()

head(aq_imputed)
```

Missing values can be mapped to a graphical element in a scatterplot by setting the `color` aesthetic in ggplot to `any_missing` (Figure \@ref(fig:track-impute-example)A), or in a density plot using the `fill = any_missing`, (Figures \@ref(fig:track-impute-example)B) and \@ref(fig:track-impute-example)C). Imputed values can also be compared to complete case data, grouping by `any_missing`, and then summarizing, similar to other dplyr summary workflows (shown below). This shows mean and median values are similar, but the minimum and maximum values are different.

```{r track-impute-example, fig.show = "hold", fig.cap = "Scatterplot (A) and density plots (B and C) of ozone and solar radiation from the airquality dataset containing imputed values from a linear model. Imputed values are colored green, and data values orange. Imputed values are similar, but slightly trended to the mean.", fig.height = 4, fig.width = 12, out.width = "100%", echo = FALSE}

p1 <- 
ggplot(aq_imputed,
       aes(x = Ozone,
           y = Solar.R,
           color = any_missing)) + 
  geom_point() +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "none") + 
  labs(tag = "A")

p2 <- 
ggplot(aq_imputed,
       aes(x = Ozone,
           fill = any_missing)) + 
  geom_density(alpha = 0.3) + 
  scale_fill_brewer(palette = "Dark2") +
  theme(legend.position = "none") + 
  labs(tag = "B")

p3 <- 
ggplot(aq_imputed,
       aes(x = Solar.R,
           fill = any_missing)) + 
  geom_density(alpha = 0.3) + 
  scale_fill_brewer(palette = "Dark2") + 
  theme(legend.position = "none") + 
  labs(tag = "C")


gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```


```{r impute-summary, echo = TRUE}
aq_imputed %>%
  group_by(any_missing) %>%
  summarise_at(.vars = vars(Ozone),
               .funs = lst(min, mean, median, max)) 
```

# Graphics {#graphics}

(note: next section to work on)

note: First sentence needs to be more on track with the overall summary of what this section is about...

- Graphics are important to understand the missingness structure.
- The multivariate relationships
- Working through a hierarchy of how to explore missings - from overview to 
- Sentence linking the two.
- How to visualise a binary matrix of missing vs not, and the dependencies between those binary variables as well.
- It is more than that, when we visualise the complete cases, we don't want to lose track of the missings themselves. This comes back to the data structure
- gg_miss_var and gg_miss_upset and vis_miss are all showing how we can summarise the missings.
- but the addition of missing counts into a histogram, allows us to understand that there are many missings - and we have a skewed distribution o Ozone. But we want to know where these values might have been located. tying this back to the nabular structure, and also the terminology of MCAR MAR, etc. (We would expect if data were MCAR for these points to be representatively thoughout the histogram)

When we define the graphics, describe which part of the nabular matrix we are going to display (overview - just binary parts), the summaries, are summaries of the binary, the histogram and points are both the missings and the complete values.
- 



- Combining that with the data itself, looking at the patterns in the complete cases, 
- from the overview through to the univariate

Visualizing missingness in a dataset is essential to understand its structure. This section discusses four different areas of visualization of missing data: overviews (\@ref(overviews)), univariate (\@ref(univariate)), bivariate (\@ref(bivariate)), and multivariate (\@ref(multivariate)). All plots are created using \pkg{ggplot2}, giving users clearer control over the plot appearance.

## Overviews {#overviews}

Rather than highlighting the amount of complete data, missing data visualizations should draw attention to the amount of missings, and ordering by missingness. Overview visualisations for variables and cases are provided with `gg_miss_var` and `gg_miss_case` (Figure \@ref(fig:gg-miss-case-var)). These are shown using the "airquality" dataset, included in base R, containing daily air quality measurements in New York, from May to September, 1973. 

```{r gg-miss-case-var, echo = FALSE, fig.show='hold', fig.cap = "Graphical summaries of missingness in variables (A) and cases (B) for the airquality data. There are missing values in Ozone and Solar.R, with Ozone having more missings. Not many cases have two missings. Most missingness is from cases with one missing value.", fig.height = 3, fig.width = 6, out.width = "90%", fig.align="center"}

p1 <- gg_miss_var(airquality)

p2 <- gg_miss_case(airquality)

cowplot::plot_grid(p1,
                   p2, 
                   ncol = 2,
                   labels = "AUTO")
```

All missings can be displayed using a heatmap style visualization. This is achieved using `vis_miss()` from the \pkg{visdat} package [@visdat], which also provides summaries of missingness overall in the legend, and for each column (Figure \@ref(fig:vis-miss)). The user can also apply clustering to the rows, and arrange columns by missingness (Figure \@ref(fig:vis-miss)B). Similar visualizations are available in other missing data packages such as \pkg{VIM}, \pkg{mi}, \pkg{Amelia}, and \pkg{MissingDataGUI}. A key improvement is `vis_miss()` orients the visualization analogous to a regular data structure: variables form columns in the visualization and are named at the top, and each row is an observation. Using \pkg{ggplot2} makes the plot easily customizable.

```{r vis-miss, fig.height = 3, fig.width = 4.5, out.width = "80%", fig.show = 'hold', echo = FALSE, dev = "png", dpi = 300, fig.cap = "Heatmap visualizations of missing data for the airquality dataset. (A) The default output and (B) ordered by clustering on rows and columns. There are only missings in ozone and solar radiation, and there appears to be some structure to their missingness."}

p1 <- vis_miss(airquality)

p2 <- vis_miss(airquality, cluster = TRUE)

cowplot::plot_grid(p1,
          p2,
          nrow = 1,
          labels = "AUTO")

```

The number of times certain variables go missing together can be visualized using an "upset plot" [@Conway2017]. Similar to a Venn diagram, an upset plot shows the size and features of overlapping sets, but scales well with more variables. Figure \@ref(fig:airquality-upset) shows only Ozone and Solar.R have missing values. The bottom right show the combinations of missingness, the bottom left showing missingness in each variable, and the top showing the combinations of missingness. The bottom left shows Ozone has the most missing values, and the top shows 2 cases where both Solar.R and Ozone go missing together.


```{r airquality-upset, fig.cap = "The pattern of missingness in the airquality dataset shown in an upset plot. Only Ozone and Solar.R have missing values, and Ozone has the most missing values. There are 2 cases where both Solar.R and Ozone have missing values.", fig.height = 3, fig.width = 4, out.width = "75%"}

gg_miss_upset(airquality)

```

## Univariate {#univariate}

Missing values are by default not shown for univariate visualizations such as histograms or densities. Two ways to use _nabular_ data to present univariate data with missings are discussed. The first imputes values below the range to facilitate visualizations. The second displays two plots of the same variable according to the missingness of a chosen variable.

**Imputing values below the range**. To visualize the amount of missings in each variable, the data is transformed into _nabular_ form, then values are imputed values below the range of data using `impute_below` (by default imputing 10% below the range). Figure \@ref(fig:impute-shift-histogram) shows a histogram of Ozone, the number of missing values on its left. Ozone is shown in a different color by referring to the variable `Ozone` as `Ozone_NA` in a fill aesthetic in ggplot.

```{r impute-shift-histogram, fig.height = 3, fig.width = 5, fig.cap = "A histogram using nabular data to show the values and missings in ozone. Values are imputed below the range to show the number of missings in Ozone and colored according to missingness of ozone (`Ozone\\_NA`). There are about 35 missings in Ozone.", error = FALSE, warning = FALSE, message = FALSE, out.width = "75%"}
  
airquality %>%
  nabular() %>%
  impute_below_all() %>%
  ggplot(aes(x = Ozone,
             fill = Ozone_NA)) + 
  geom_histogram() + 
  scale_fill_brewer(palette = "Dark2") + 
  theme_minimal()
```

**Univariate split by missingness**. Missingness of one variable can also be used to display different distributions in another. Figure \@ref(fig:bind-shadow-density) shows the values of temperature when ozone is present, and missing. Figure \@ref(fig:bind-shadow-density)A is a faceted histogram, and Figure \@ref(fig:bind-shadow-density)B is an overlaid density. This shows how values of temperature are affected by the missingness of ozone, and reveals a cluster of low temperature observations with missing ozone values.

```{r bind-shadow-density, fig.height = 3, fig.width = 5, out.width = "49%", error = FALSE, message = FALSE, fig.cap = "Temperature according to missingness in ozone from in the airquality dataset. A histogram of temperature facetted by the missingness of ozone (A), or a density of temperature colored by missingness in ozone (B). These show a cluster of low temperature observations with missing ozone values, but temperature is otherwise similar.", fig.show = 'hold'}

airquality %>%
  nabular() %>%
ggplot(aes(x = Temp)) + 
  geom_histogram(na.rm = TRUE) + 
  facet_wrap(~Ozone_NA) + 
  labs(tag = "A")

airquality %>%
  nabular() %>%
ggplot(aes(x = Temp,
           colour = Ozone_NA)) + 
  geom_density(na.rm = TRUE) +
  scale_colour_brewer(palette = "Dark2")  + 
  labs(tag = "B")

```


## Bivariate {#bivariate}

To visualize missing values in two dimensions the missing values can be placed in plot margins, by imputing values below the range of the data.  Using _nabular_ data identifies imputed values (\@ref(univariate-graphics)), and color makes missingness pre-attentive [@treisman1985]. The steps of imputing and coloring are combined into `geom_miss_point()`. Figure \@ref(fig:geom-miss) shows a mostly uniform spread of missing values for Solar.R and Ozone. As `geom_miss_point()` is a defined \pkg{ggplot2} geometry, it works with features such as facetting and mapping other variables to graphical aesthetics. The shadow format by itself is also useful for exploring patterns of missingness. 


```{r geom-miss, fig.height = 3, fig.width = 6, fig.cap = "Scatterplots with missings displayed at 10 percent below for the airquality dataset. Scatterplots of ozone and solar radiation (A), and ozone and temperature (B). There are missings in ozone and solar radiation, but not temperature."}

p1 <-
ggplot(data = airquality,
       aes(x = Ozone,
           y = Solar.R)) + 
  geom_miss_point() + 
  scale_colour_brewer(palette = "Dark2") + 
  theme(legend.position = "bottom") + 
  labs(tag = "A")

p2 <- 
ggplot(data = airquality,
       aes(x = Temp,
           y = Ozone)) + 
  geom_miss_point() + 
  scale_colour_brewer(palette = "Dark2") +
  theme(legend.position = "bottom")  + 
  labs(tag = "B")

gridExtra::grid.arrange(p1, p2, ncol = 2)

```


```{r geom-miss-point-impute-shift-long, include = FALSE, out.width = "70%"}
airquality %>%
  nabular() %>%
  impute_below_all() %>%
  add_label_shadow() %>%
  ggplot(aes(x = Ozone,
             y = Solar.R,
             colour = any_missing)) + 
  geom_point() + 
  scale_colour_brewer(palette = "Dark2") + 
  theme(legend.position = "bottom")
```


## Multivariate {#multivariate}

Parallel coordinate plots can visualize missingness beyond two dimensions. They transform variables to the same scale, ranging between 0 and 1. The `oceanbuoys` dataset from \pkg{naniar} is used for this visualisation, containing measurements of ocean buoys to understand and predict El Nio and El Nia. Data was collected in 1993 and 1997, and contains measurements of sea and air temperature, humidity, and east west and north south wind speeds. Figure \@ref(fig:parallel-cord-plot) shows a parallel coordinate plot of the `oceanbuoys`, with missing values imputed to be 10% below the range, and values colored according to where humidity was missing (humidity_NA). This shows humidity is missing at low air and sea temperatures, and humidity is missing in one year, and one location. 

```{r parallel-cord-plot, fig.width = 8, fig.height = 4, out.width = "100%", echo = FALSE, fig.cap = "Parallel coordinate plot shows missing values imputed 10\\% below range for the oceanbuoys dataset. Values are colored by missingness of humidity. Humidity is missing for low air and sea temperatures, and is missing for one year and one location. "}

range_01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

dat_paral <-  oceanbuoys %>%
  mutate(ID = 1:n(),
         ID = as.factor(ID)) %>%
  nabular() %>%
  impute_below_all() %>%
  add_label_shadow() %>%
  mutate_if(is.numeric, range_01) %>%
  gather(key, value, -c(9:19)) %>%
  select(ID, key, value, everything())

ggplot(dat_paral,
       aes(x = key,
           y = value,
           group = ID,
           colour = humidity_NA)) + 
  geom_line(alpha = 0.3) + 
  theme(legend.position = "bottom") + 
  scale_colour_brewer(palette = "Dark2")

```

# Numerical summaries {#num-sum}

This section describes approaches to summarizing missingness. The functions are implemented in \pkg{naniar}, and are easy to remember as they have consistent names and output, returning either a single number \@ref(single-num-sum), or a dataframe (\@ref(sum-tab-missings)), integrating well with plotting and modelling tools. How these work with other tools in an analysis pipeline is shown in \@ref(num-sum-w-group).


## Single number summaries {#single-num-sum}

The overall number, proportion, or percent of missing values in a dataset is shown with `n_miss`, `prop_miss` and `pct_miss`, and their complete value complements: `n_complete`, `prop_complete`, and `pct_complete`. Summaries for variables and cases are made by appending `_case` or `_var` to any of the summaries. An overview is shown in Table \@ref(tab:n-prop-pct-miss-complete).

```{r n-prop-pct-miss-complete, echo = FALSE}

table_of_fun <- tibble::tibble(
  "Missing Function" = c("n_miss", 
                         "prop_miss", 
                         "pct_miss",
                         "pct_miss_case",
                         "pct_miss_var"),
  "missing value" = c(n_miss(airquality),
                      prop_miss(airquality),
                      pct_miss(airquality),
                      pct_miss_case(airquality),
                      pct_miss_var(airquality)),
  "Complete function" = c("n_complete",
                          "prop_complete",
                          "pct_complete",
                          "prop_complete_case",
                          "pct_complete_var"),
  "complete value" = c(n_complete(airquality), 
                       prop_complete(airquality), 
                       pct_complete(airquality),
                       pct_complete_case(airquality), 
                       pct_complete_var(airquality))
) %>%
  mutate_if(is.numeric,round,2)

knitr::kable(
  table_of_fun, 
  digits = 2,
  caption = "Single number summaries of missingness and completeness of the airquality dataset. The functions follow consistent naming, making them easy to remember, and their use clear.")
  # booktabs = TRUE)
  # kable_styling(latex_options = c("hold_position"))

```

## Summaries and tabulations of missing data {#sum-tab-missings}

Presenting the number and percent of missing values for each variable, or case, provides a summary usable in models to inform imputations or data handling. For example, potentially dropping variables or deciding to include others in an imputation model. Another useful approach is to tabulate the frequency of missing values for each variable or case; that is, the number of times there are zero, one, two, and so on, missing values. These summaries and tabulations are shown for variable in Tables \@ref(tab:miss-var-summary) and \@ref(tab:miss-var-table), using `miss_var_summary` and `miss_var_table`. Functions are also provided for case-wise summaries and tabulations: `miss_case_summary` and `miss_case_table`, which order rows by `n_miss`, to show the most missings at the top. `naniar` also shows missingness summaries across a repeating span, `miss_var_span`, and finding streaks or runs of missingness in a given variable in `miss_var_run`.

```{r miss-var-summary}

miss_var_summary(airquality) %>% 
  knitr::kable(
    caption = "\\texttt{miss\\char`_var\\char`_summary} provides the number and percent of missings in each variable in airquality. Only ozone and solar radiation have missing values.",
    digits = 1)
    # booktabs = TRUE) 
  # kable_styling(latex_options = c("hold_position"))

```

```{r miss-var-table}

miss_var_table(airquality) %>% 
  knitr::kable(
    caption = "\\texttt{miss\\char`_var\\char`_table} tabulates the amount of missing data in each variable in airquality. This shows the number of variables with 0, 7, and 37 missings, and the percentage of variables with those amounts of missingness. There are few missingness patterns.",
    digits = 1)
    # booktabs = TRUE)
  # kable_styling(latex_options = c("hold_position"))
```

## Combining numerical summaries with grouping operations {#num-sum-w-group}

These summaries and tabulations can be composed with the "pipe" operator to perform grouped summaries with `dplyr`'s `group_by` function.
These summaries return dataframes, easily used in other plotting devices, modelling tools, or other summaries. Table \@ref(tab:group-miss-var-summary) shows an example of missing data summaries for airquality by each month.

```{r group-miss-var-summary}

airquality %>%
  group_by(Month) %>%
  miss_var_summary() %>%
  ungroup() %>%
  slice(1:10) %>%
  knitr::kable(caption = "\\texttt{miss\\char`_var\\char`_summary} combined with \\texttt{group\\char`_by} provides a grouped summary of the missingness in each variable, for each Month of the airquality dataset. Only the first 10 rows are shown. There are more ozone missings in June than May.",
               digits = 1)
               # booktabs = TRUE)
  # kable_styling(latex_options = c("hold_position"))

```



# Application {#case-study}

```{r data-setup, include = FALSE}

housing_raw <-
  readr::read_csv(here::here("data",
                             "melbourne_housing_raw.csv")) %>%
  janitor::clean_names() %>%
  dplyr::rename(region_name = regionname,
         property_count = propertycount) %>%
  dplyr::mutate(date = lubridate::dmy(date)) %>%
  dplyr::rename(latitude = lattitude,
                longitude = longtitude) %>%
  # let's create monthly quarters
  mutate(yr_qtr = as.ordered(lubridate::quarter(date, 
                                             with_year = TRUE,
                                             fiscal_start = 1))) %>%
  # drop price
  tidyr::drop_na(price) %>%
  # make price the log of price
    mutate(price = log10(price))
  

```

```{r data-clean, include = FALSE}

# compact down the seller levels
count_dat <- count(housing_raw,seller_g) %>% 
  arrange(-n) %>%
  mutate(cumu = cumsum(n),
         pct = cumu / nrow(housing_raw),
         pct_change = pct - lag(pct)) %>%
  tibble::rowid_to_column() %>%
  mutate(seller = case_when(
    pct < 0.52 ~ as.character(seller_g),
    pct > 0.52 & pct < 0.60 ~ "seller_g1",
    pct > 0.60 & pct < 0.65 ~ "seller_g2",
    pct > 0.65 & pct < 0.70 ~ "seller_g3",
    pct > 0.70 & pct < 0.75 ~ "seller_g4",
    pct > 0.75 & pct < 0.80 ~ "seller_g5",
    pct > 0.80 & pct < 0.85 ~ "seller_g6",
    pct > 0.85 & pct < 0.90 ~ "seller_g6",
    pct > 0.90 & pct < 0.95 ~ "seller_g6",
    pct > 0.95 ~ "seller_g5"
  ))

# drop these three variables, as we won't use them in analysis, they
# would need extensive recoding

housing <- housing_raw %>%
  mutate_if(is.character, as.factor) %>%
  mutate_at(vars(rooms,
                 bedroom2,
                 bathroom,
                 car),
            as.factor) %>%
  # recode / collapse factors with many levels and few obs
  mutate(bathroom = fct_other(bathroom,
                                drop = c("4", "5", "6", "7", "8", "9"),
                                other_level = "4+")) %>%
  mutate(bedroom2 = fct_other(bedroom2,
                               drop = c("5", "6", "7", "8", "9", "10", 
                                        "12", "16", "20"),
                               other_level = "5+")) %>%
  mutate(rooms = fct_other(rooms,
                             drop = c("6", "7", "8", "9", "10", 
                                      "12", "16"),
                             other_level = "6+")) %>%
  mutate(car = fct_other(car,
                           drop = c("4", "5", "6", "7", "8", "9", "10",
                                    "11", "18"),
                           other_level = "4+")) %>%
  left_join(select(count_dat, seller_g, seller), by = "seller_g")  %>%
  # drop seller_g
  select(-seller_g) %>%
  mutate(seller = as.factor(seller))
  

```

To illustrate the methods, data on housing for the city of Melbourne from January 28, 2016 to March 17, 2018 is used. The data was compiled by scraping weekly property clearance data [@Kaggle-2018-data]. There are `r scales::number(nrow(housing), big.mark = ",")` properties, and `r ncol(housing)` variables in the dataset. The variables include real estate type (town house, unit, house), suburb, selling method, number of rooms, price, real estate agent, sale date, and distance from Central Business District (CBD).

The goal in analyzing this data is to accurately predict Melbourne housing prices. The data contains many missing values. As a precursor to building a predictive model, this analysis focusses on understanding the patterns of missingness. This section shows how the methods from previous sections are used together in a data analysis workflow.

## Exploring patterns of missingness {#case-study-explore-pattern}

Figure \@ref(fig:housing-miss-case-var)A shows 9 variables with missing values. The most missings are in building area, followed by year built, and land size, with similar amounts of missingness in Car, bathroom, bedroom2, longitude, and latitude. Figure \@ref(fig:housing-miss-case-var)B reveals there are up to 50% missing values in cases, and the majority of cases have more than 5% values missing. The variables building area and year built have more than 50% missing data, and so could perhaps be omitted from subsequent analysis, as imputed values are likely to be spurious. Three missingness clusters are revealed by visualizing missingness in the whole dataset, clustering and arranging the rows and columns of the data (Figure \@ref(fig:applic-vis-miss)).


```{r housing-miss-case-var, fig.cap = "The amount of missings in variables (A) and cases (B) for Melbourne housing data. (A) Build area and year built have more than 50\\% missing, and car, bathroom, bedroom2 and longitude and latitude have about 25\\% missings. (B) Cases are missing 5 - 50\\% of values. The majority of missingness is in selected cases and variables.", fig.height = 4, fig.width = 8, out.width = "100%"}

q1 <- gg_miss_var(housing, show_pct = TRUE) + labs(tag = "A")

q2 <- gg_miss_case(housing, show_pct = TRUE) + labs(tag = "B")

gridExtra::grid.arrange(q1, q2, ncol = 2)

```


```{r applic-vis-miss, fig.height = 4, fig.width = 6, out.width = "85%", fig.show='hold', fig.cap = "Heatmap of clustered missingness for housing data reveals structured missingness. Three groups of missingness are apparent. At the top: building area to longitude; the middle: building area and year built; the end: building area, year built, and landsize.", dev = "png", dpi = 300}

housing %>%
  # sample_frac(0.25) %>%
  vis_miss(cluster = TRUE,
           sort_miss = TRUE,
           show_perc_col = FALSE)

```

Figure \@ref(fig:applic-vis-miss) shows missingness patterns with an `upset` plot [@Conway2017], displaying 8 intersecting sets of missing variables. Two patterns stand out: two, and five variables missing, providing further evidence of the missingness patterns seen in Figures \@ref(fig:housing-miss-case-var) and \@ref(fig:housing-upset).


```{r housing-upset, fig.height = 5, fig.width = 8, fig.cap = "An upset plot of 8 sets of missingness in the housing data. Missingness for each variable is shown on the bottom left. Connected dots show co-occurences of missings in variables. Two missingness patterns are clear, year built and building area, and lattitude through to building area.", out.width = "100%"}

gg_miss_upset(housing, 
              nsets = 8,
              order.by = "freq")

```


Tabulating the number of missings in variables in Table \@ref(tab:housing-miss-var-case-table) (left) shows three groups of missingness. Tabulating missings in cases (Table \@ref(tab:housing-miss-var-case-table) (right) shows six patterns of missingness. These overview plots lead to the removal of two variables from with more than 50% missingness from analysis: building area and year built.


```{r housing-miss-var-case-table}

t1 <-  miss_var_table(housing) 
t2 <- miss_case_table(housing)


knitr::kable(list(t1, t2),
             caption = "Tabulating missingness for variables (left) and cases (right) to understand missingness patterns. 14 variables have 0 - 3 missings, 6 variables have 6000 - 9000 missings, and 2 variables have 15 - 16,000 missings. About 30\\% of cases have no missings, 45\\% of cases have 1 - 6 missings, and about 23\\% of cases have 8 or more missings. There are different patterns of missingness in variables and cases, but they can be broken down into smaller groups.",
             digits = 1)
             # booktabs = TRUE)
  # kable_styling(latex_options = c("hold_position"))

```


## Exploring missingness patterns for imputation {#case-study-explore-for-imp}

Using information from \@ref(case-study-explore-pattern), the following variables are explored for features predicting missingness: Land size, latitude, longitude, bedroom2, bathroom, car, and land size.

Missingness structure is explored by clustering the missing values into groups. Then, a  classification and regression trees (CART) model is applied to predict these missingness clusters using the remaining variables, to find variables and values that predicting missingness clusters [@Tierney2015; @Barnett2017]. Two clusters of missingness are identified and predicted using all variables in the dataset with the CART package `rpart` [@rpart], and plotted using the `rpart.plot` package [@rpart-plot].  Importance scores reveal variables important for predicting missingness: rooms, price, suburb, council area, distance, and region name. These variables are important for predicting missingness, so are important to include in the imputation model. 

```{r housing-cluster-miss}
housing_cls <- add_miss_cluster(housing, n_clusters = 2)
```

```{r rpart-fit-cluster}

miss_cls_pre_fit <- housing_cls %>%
  mutate(year = lubridate::year(date),
         quarter = lubridate::quarter(date)) %>%
  mutate_at(vars(year, quarter),
            as.factor) %>%
  select(suburb,
         council_area,
         postcode,
         method, 
         region_name,
         rooms,
         seller,
         type,
         year,
         quarter,
         price,
         property_count,
         distance,
         latitude,
         longitude,
         miss_cluster)

miss_cls_fit_rpart <- rpart(factor(miss_cluster) ~ ., 
                            data = miss_cls_pre_fit)

```

```{r rpart-plot, fig.height = 4, fig.width = 8, out.width = "90%", fig.align = "center", fig.cap = "Decision tree output predicting missingness clusters. Type of house, year quarter, and year were important for predicting missingness cluster. The cluster with the most missingness was for quarters 1 and 4, for 2017 and 2018. Type of house, year, and year quarter are important features related to missingness structure."}

rpart.plot::prp(miss_cls_fit_rpart,
                type = 4,
                extra = 2,
                fallen.leaves = TRUE,
                prefix = "cluster = ",
                suffix = " \nObservations",
                box.col = "lightgrey",
                border.col = "grey",
                branch.col = "grey40")

```

```{r vis-miss-each-node, fig.width = 3, fig.height = 3, out.width = "49%", fig.show='hold'}

data_in <- housing_cls %>%
  mutate(year = lubridate::year(date),
         month = lubridate::month(date),
         quarter = lubridate::quarter(date)) %>%
  mutate_at(vars(year, month, quarter),
            as.factor)

node_1 <- data_in %>% 
  filter(type == "h")

node_2 <- data_in %>% 
  filter(type != "h",
         quarter == 2 | quarter == 3) 

node_3 <- data_in %>%
  filter(type != "h",
         quarter == 1 | quarter == 4,
         year == 2016)

node_4 <- data_in %>%
  filter(type != "h",
         quarter == 1 | quarter == 4,
         year == 2017 | year == 2018)

```


## Imputation and diagnostics {#case-study-imp-diagnosis}

\pkg{simputation} is used to implement two imputation: simple linear regression and K nearest neighbors. Values are imputed stepwise in ascending order of missingness. The  track missings pattern is applied (described in \@ref(verbs-track)), to assess imputed values. Imputed datasets are compared on their performance in a model predicting log house price for 4 variables (Figure \@ref(fig:imputed-by-model)). Compared to KNN imputed values, the linear model imputed values closer to the mean. 

```{r clean-data-for-analysis, include = FALSE, cache = TRUE}
# add memoise to store the output data for this
impute_knn <- simputation::impute_knn
dat_house <- housing_cls %>%
  mutate(year = lubridate::year(date),
         # month = lubridate::month(date),
         quarter = lubridate::quarter(date)) %>%
  mutate_at(vars(car, bathroom, bedroom2), as.integer)

dat_house_knn <- dat_house %>% 
  nabular() %>%
  as.data.frame() %>% 
  # postcode
  impute_knn(postcode ~ type + quarter + year + rooms + price + distance) %>%
  # distance
  impute_knn(distance ~ type + quarter + year + rooms + price) %>%
  # property_count
  impute_knn(property_count ~ type + quarter + year + rooms + price + distance) %>%
  # longitude
  impute_knn(longitude ~ type + quarter + year + rooms + price + distance) %>%
  # latitude
  impute_knn(latitude ~ type + quarter + year + rooms + price + distance) %>%
  # bedroom2
  impute_knn(bedroom2 ~ type + quarter + year + rooms + price + distance) %>%
  # bathroom
  impute_knn(bathroom ~ type + quarter + year + rooms + price + distance) %>%
  # car
  impute_knn(car ~ type + quarter + year + rooms + price + distance) %>%
  # landsize
  impute_knn(landsize ~ type + quarter + year + rooms + price + distance) %>%
  add_label_shadow()

```


```{r impute-knn, cache = TRUE}

dat_house_lm <- dat_house %>%
  nabular() %>%
  as.data.frame() %>%
  # postcode
  impute_knn(postcode ~ type + quarter + year + rooms + price + distance) %>%
  # distance
  impute_lm(distance ~ type + quarter + year + rooms + price) %>%
  # property_count
  impute_lm(property_count ~ type + quarter + year + rooms + price + distance) %>%
  # longitude
  impute_lm(longitude ~ type + quarter + year + rooms + price + distance) %>%
  # latitude
  impute_lm(latitude ~  type + quarter + year + rooms + price + distance) %>%
  # bedroom2
  impute_lm(bedroom2 ~  type + quarter + year + rooms + price + distance) %>%
  # bathroom
  impute_lm(bathroom ~  type + quarter + year + rooms + price + distance) %>%
  # car
  impute_lm(car ~ type + quarter + year + rooms + price + distance) %>%
  # landsize
  impute_lm(landsize ~  type + quarter + year + rooms + price + distance) %>%
  add_label_shadow() %>%
  # recode integers back
  mutate_at(vars(bedroom2, bathroom, car), as.integer)


```



```{r compare-assess-imputations}

dat_house_cc <- na.omit(dat_house)

bound_models <- bind_rows(lm = dat_house_lm,
                          knn = dat_house_knn,
                          cc = dat_house_cc,
                          .id = "imp_model") %>%
  as_tibble()

```


```{r imputed-by-model, fig.cap = "Boxplots of complete case data, and data imputed with KNN or linear model for different variables. (A) number of bedrooms, (B) number of bathrooms, (C) number of carspots, and (D) landsize (on a log10 scale). KNN had similar results to complete case, and linear model had a lower median for cars and fewer extreme values for bedrooms.", fig.height = 2.5, fig.width = 7, out.width = "95%"}

# Now we want to look at the distribution of the values, so we need to do
# some gathering
bound_models_gather <- bound_models %>%
  select(
         bedroom2,
         bathroom,
         car,
         landsize,
         any_missing,
         imp_model) %>%
  gather(key = "key",
         value = "value",
         -any_missing,
         -imp_model)

p1 <- bound_models %>%
  ggplot(aes(x = imp_model,
             y = bedroom2)) + 
  geom_boxplot()

p2 <- bound_models %>%
  ggplot(aes(x = imp_model,
             y = bathroom)) + 
  geom_boxplot()
  
p3 <- bound_models %>%
  ggplot(aes(x = imp_model,
             y = car)) + 
  geom_boxplot()

p4 <- bound_models %>%
  ggplot(aes(x = imp_model,
             y = landsize)) + 
  geom_boxplot() + 
  scale_y_log10()
  
cowplot::plot_grid(p1,
          p2,
          p3,
          p4,
          nrow = 1,
          labels = "AUTO")

```



```{r SUPP-FIG-distribution-imputed-vals-all, fig.width = 6, fig.height = 6, out.width = "100%", eval = FALSE}

bound_models_gather %>%
  filter(any_missing == "Missing") %>%
  ggplot(aes(x = imp_model,
             y = value,
             colour = imp_model)) +
  geom_boxplot() + 
  facet_wrap(~key, scales = "free_y") + 
    theme(legend.position = "bottom")


```


## Assess model predictions {#case-study-assess-model}

```{r fit-each-model}

dat_fit <- dat_house %>%
  mutate_at(vars(car,
                 bathroom,
                 bedroom2,
                 type,
                 year,
                 quarter,
                 rooms),
            .funs = as.factor)

dat_fit_knn <-  dat_house_knn %>%
  mutate_at(vars(car,
                 bathroom,
                 bedroom2,
                 type,
                 year,
                 quarter,
                 rooms),
            .funs = as.factor)

dat_fit_lm <-  dat_house_lm %>%
  mutate_at(vars(car,
                 bathroom,
                 bedroom2,
                 type,
                 year,
                 quarter,
                 rooms),
            .funs = as.factor)

dat_house_cc <- na.omit(dat_fit)

fit_lm_dat_house_cc <- lm(price ~ landsize + 
                            car + 
                            bathroom + 
                            bedroom2 +
                            type + 
                            year + 
                            quarter +
                            rooms +
                            distance,
                          dat_house_cc)

fit_lm_dat_house_knn <- lm(price ~ landsize + 
                            car + 
                            bathroom + 
                            bedroom2 +
                            type + 
                            year + 
                            quarter +
                            rooms +
                            distance,
                           dat_fit_knn)

fit_lm_dat_house_lm <- lm(price ~ landsize + 
                            car + 
                            bathroom + 
                            bedroom2 +
                            type + 
                            year + 
                            quarter +
                            rooms +
                            distance,
                          dat_fit_lm)

```

Coefficients of the linear model of log price vary for room for different imputed datasets (Figure \@ref(fig:tidy-coefs)). Notably, complete cases underestimate the impact of room on log price. A partial residual plot (Figure \@ref(fig:partial-resid)) shows there is not much variation amongst the models from the differently imputed datasets.

```{r tidy-coefs, fig.height = 2, fig.width = 7, fig.cap = "The coefficient estimate for the number of rooms varies according to the imputed dataset. Complete case dataset produced lower coefficients, compared to imputed datasets", out.width = "100%"}

tidy_fit_cc <- broom::tidy(fit_lm_dat_house_cc)
tidy_fit_knn <- broom::tidy(fit_lm_dat_house_knn)
tidy_fit_lm <- broom::tidy(fit_lm_dat_house_lm)

tidy_fits <- bind_rows(cc = tidy_fit_cc,
                       knn = tidy_fit_knn,
                       lm = tidy_fit_lm,
                       .id = "model")

tidy_fits_data <-  tidy_fits %>%
  filter(term != "(Intercept)") %>%
  filter(grepl("rooms", term)) %>%
  select(model, term, estimate) %>%
  spread(key = model,
         value = estimate)

tidy_fits %>%
  filter(term != "(Intercept)") %>%
  filter(grepl("rooms", term)) %>%
  select(model, term, estimate) %>%
  ggplot(aes(x = estimate,
             y = term,
             colour = model)) + 
  geom_point() +
  theme_minimal() +
  labs(x = "Estimate",
       y = "Term") + 
  guides(colour = guide_legend(title = "Model"))

```

```{r partial-resid, fig.cap = "Partial residual plot for each data set, complete cases (cc), and imputed with KNN (knn) or linear model (lm). These are plotted as hex bins, colored according to the number of points in a given hexagon. Brighter colors mean more points. Compared to complete cases, imputed data has more points clustered around zero.", fig.width = 9, fig.height = 3, out.width = "100%"}

aug_fit_cc <-  broom::augment(fit_lm_dat_house_cc) %>% as_tibble()
aug_fit_knn <- broom::augment(fit_lm_dat_house_knn) %>% as_tibble()
aug_fit_lm <-  broom::augment(fit_lm_dat_house_lm)%>% as_tibble()

aug_fits <- bind_rows(cc =  aug_fit_cc,
                      knn = aug_fit_knn,
                      lm =  aug_fit_lm,
                      .id = "model")

gg_hex <- function(data,
                   subset){
  
  data_subset <- data %>% 
    filter(model == subset)
  
    ggplot(data_subset, aes(x = .fitted,
               y = .resid)) + 
    geom_hex() + 
    scale_fill_viridis_c() +
    theme(legend.position = "bottom",
          aspect.ratio = 1) +
      labs(tag = subset)
}

gg_hex(aug_fits, "cc") +
gg_hex(aug_fits, "knn") +
gg_hex(aug_fits, "lm")


```

## Summary {#case-study-summary}

The `naniar` and `visdat` packages implement the methods discussed in the paper, building on existing tidy tools and strike a compromise between automation and control, making analysis efficient, readable, but not overly complex. Each tool has clear intent and effects - plotting or generating data or augmenting data in some way. This reduces repetition and typing for the user, making exploration of missing values easier as they follow consistent, predictable rules.

# Discussion {#discussion}

This paper has described new methods for exploring, visualizing, and imputing missing data. The work was motivated by recent developments of tidy data, and extends them for better missing value handling. The methods have standard outputs, function arguments, and behavior. This provides consistent workflows centered around data analysis that integrate well with existing imputation methodology, visualization, and modelling. 

The _nabular_ data structures discussed in the paper are simple by design, to promote flexibility.  They could be used to create different visualizations than were shown in the paper. The analyst can use the data structures to decide on appropriate visualization for their problem. The data structures could also be used to support interactive graphics, in the manner of MANET and ggobi. Linking the plots (via linked brushing) could facilitate exploration of missingness, and could be implemented with \pkg{plotly} [@plotly] for added interactivity. Animating between different sets of imputed values could also be possible with packages like \pkg{gganimate} [@gganimate].

Other data structures such as spatial data, time series, networks, and
longitudinal data would be supported by the inherently tabular,
_nabular_ data, if they are first structured as wide tidy format. Large data may need special handling, and additional features like efficient storage of purely imputed values and lazy evaluation. Special missing value codes could be improved by creating special classes, or expanding low level representation of `NA` at the source code level.

The methodology described in this paper can be used in conjunction with other approaches to understand multivariate missingness dependencies (e.g. decision trees [@Tierney2015], latent group analysis [@Barnett2017], and PCA [@FactoMineR]) . Evaluating imputed values using a testing framework like @VanBuuren2012 is also supported. 

The approach meshes with the dynamic nature of data analysis, allowing the analyst to go from raw data to model data in a fluid workflow.

# Acknowledgements

The authors would like to thank Miles McBain, for his key contributions and discussions on the `naniar` package, in particular for helping implement `geom_miss_point`, and for his feedback on ideas, implementations, and names. We also thank Colin Fay for his contributions to the `naniar` package, in particular for his assistance with the `replace_with_na` functions. We also thank Earo Wang and Mitchell O'Hara-Wild for the many useful discussions on missing data and package development, and for their assistance with creating elegant approaches that take advantage of the tidy syntax. We would also like to thank those who contributed pull requests and discussions on the `naniar` package, in particular Jim Hester and Romain Franois for improving the speed of key functions, Ross Gayler for discussion on special missing values, and Luke Smith for helping `naniar` be more compliant with `ggplot2`. We would also like to thank Amelia McNamara for her assistance with proof-reading the paper.


# References
